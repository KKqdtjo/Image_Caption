import torch
import pickle
import json
import os
from tqdm import tqdm
import argparse
from collections import defaultdict
import nltk
from nltk.translate.bleu_score import corpus_bleu, sentence_bleu
from nltk.translate.meteor_score import meteor_score
from rouge_score import rouge_scorer
import matplotlib.pyplot as plt
from PIL import Image
import random

# è§£å†³OpenMPå†²çªé—®é¢˜
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
matplotlib.rcParams['axes.unicode_minus'] = False

from dataset import get_data_loader, Vocabulary
from model import ImageCaptioningModel

# ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')

def generate_captions_for_eval(model, data_loader, vocab, device):
    """ä¸ºè¯„ä¼°ç”Ÿæˆæ‰€æœ‰å›¾åƒçš„æè¿°"""
    model.eval()
    generated_captions = []
    reference_captions = []
    
    with torch.no_grad():
        for images, captions, lengths in tqdm(data_loader, desc="ç”Ÿæˆæè¿°"):
            images = images.to(device)
            
            # ç”Ÿæˆæè¿°
            sampled_ids = model.sample(images)
            
            for i in range(images.size(0)):
                # è½¬æ¢ç”Ÿæˆçš„æè¿°
                generated = []
                for word_id in sampled_ids[i].cpu().numpy():
                    word = vocab.idx2word[word_id]
                    if word == '<end>':
                        break
                    if word not in ['<start>', '<pad>']:
                        generated.append(word)
                
                # è½¬æ¢çœŸå®æè¿°
                reference = []
                caption_length = lengths[i] if isinstance(lengths, list) else lengths[i].item()
                for word_id in captions[i][:caption_length].cpu().numpy():
                    word = vocab.idx2word[word_id]
                    if word == '<end>':
                        break
                    if word not in ['<start>', '<pad>']:
                        reference.append(word)
                
                generated_captions.append(generated)
                reference_captions.append([reference])  # BLEUéœ€è¦å‚è€ƒå¥å­åˆ—è¡¨
    
    return generated_captions, reference_captions

def calculate_bleu_scores(generated_captions, reference_captions):
    """è®¡ç®—BLEUåˆ†æ•°"""
    # è®¡ç®—corpus-level BLEU
    bleu_1 = corpus_bleu(reference_captions, generated_captions, weights=(1, 0, 0, 0))
    bleu_2 = corpus_bleu(reference_captions, generated_captions, weights=(0.5, 0.5, 0, 0))
    bleu_3 = corpus_bleu(reference_captions, generated_captions, weights=(0.33, 0.33, 0.33, 0))
    bleu_4 = corpus_bleu(reference_captions, generated_captions, weights=(0.25, 0.25, 0.25, 0.25))
    
    return {
        'BLEU-1': bleu_1,
        'BLEU-2': bleu_2,
        'BLEU-3': bleu_3,
        'BLEU-4': bleu_4
    }

def calculate_meteor_scores(generated_captions, reference_captions):
    """è®¡ç®—METEORåˆ†æ•°"""
    meteor_scores = []
    
    for gen, refs in zip(generated_captions, reference_captions):
        # METEORéœ€è¦å­—ç¬¦ä¸²æ ¼å¼
        gen_str = ' '.join(gen)
        ref_str = ' '.join(refs[0])
        
        try:
            score = meteor_score([ref_str], gen_str)
            meteor_scores.append(score)
        except:
            meteor_scores.append(0.0)
    
    return sum(meteor_scores) / len(meteor_scores)

def calculate_rouge_scores(generated_captions, reference_captions):
    """è®¡ç®—ROUGEåˆ†æ•°"""
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    
    rouge_scores = defaultdict(list)
    
    for gen, refs in zip(generated_captions, reference_captions):
        gen_str = ' '.join(gen)
        ref_str = ' '.join(refs[0])
        
        scores = scorer.score(ref_str, gen_str)
        
        rouge_scores['ROUGE-1'].append(scores['rouge1'].fmeasure)
        rouge_scores['ROUGE-2'].append(scores['rouge2'].fmeasure)
        rouge_scores['ROUGE-L'].append(scores['rougeL'].fmeasure)
    
    return {
        'ROUGE-1': sum(rouge_scores['ROUGE-1']) / len(rouge_scores['ROUGE-1']),
        'ROUGE-2': sum(rouge_scores['ROUGE-2']) / len(rouge_scores['ROUGE-2']),
        'ROUGE-L': sum(rouge_scores['ROUGE-L']) / len(rouge_scores['ROUGE-L'])
    }

def show_visual_results_single_image(model, data_loader, vocab, device, img_dir, split_name):
    """å±•ç¤ºå•å¼ å›¾ç‰‡çš„å¤šä¸ªæ ‡æ³¨å¯¹æ¯”"""
    print(f"\nğŸ“¸ å•å›¾å¤šæ ‡æ³¨å±•ç¤ºæ¨¡å¼...")
    
    model.eval()
    
    # æ”¶é›†ä¸€å¼ å›¾ç‰‡çš„å¤šä¸ªæ ‡æ³¨
    with torch.no_grad():
        for images, captions, lengths in data_loader:
            images = images.to(device)
            sampled_ids = model.sample(images)
            
            # åªå–ç¬¬ä¸€å¼ å›¾ç‰‡
            image = images[0]
            dataset = data_loader.dataset
            filename = dataset.data[0]['filename']
            
            # è·å–AIç”Ÿæˆçš„æè¿°
            generated = []
            for word_id in sampled_ids[0].cpu().numpy():
                word = vocab.idx2word[word_id]
                if word == '<end>':
                    break
                if word not in ['<start>', '<pad>']:
                    generated.append(word)
            ai_caption = ' '.join(generated)
            
            # è·å–è¿™å¼ å›¾ç‰‡çš„æ‰€æœ‰äººå·¥æ ‡æ³¨
            human_captions = []
            for sentence in dataset.data[0]['sentences']:
                human_captions.append(' '.join(sentence['tokens']))
            
            # åˆ›å»ºå±•ç¤º
            fig = plt.figure(figsize=(16, 12))
            
            # å·¦ä¾§æ˜¾ç¤ºå›¾åƒ
            ax_img = plt.subplot2grid((3, 2), (0, 0), rowspan=3)
            
            img_path = os.path.join(img_dir, filename)
            if os.path.exists(img_path):
                original_image = Image.open(img_path).convert('RGB')
                ax_img.imshow(original_image)
                ax_img.axis('off')
                ax_img.set_title(f'å›¾åƒ: {filename}', fontsize=14, fontweight='bold', pad=20)
            
            # å³ä¾§æ˜¾ç¤ºAIç”Ÿæˆå’Œäººå·¥æ ‡æ³¨
            ax_text = plt.subplot2grid((3, 2), (0, 1), rowspan=3)
            ax_text.axis('off')
            
            # æ„å»ºæ–‡æœ¬å†…å®¹
            text_content = f" æ¨¡å‹ç”Ÿæˆæè¿°:\n{ai_caption}\n\n"
            text_content += f" äººå·¥æ ‡æ³¨ (å…±{len(human_captions)}æ¡):\n"
            for idx, caption in enumerate(human_captions, 1):
                text_content += f"{idx}. {caption}\n"
            
            ax_text.text(0.05, 0.95, text_content, 
                        transform=ax_text.transAxes,
                        fontsize=12, va='top', ha='left',
                        bbox=dict(boxstyle="round,pad=1", 
                                facecolor="lightyellow", 
                                alpha=0.9,
                                edgecolor="orange",
                                linewidth=2))
            
            plt.suptitle('å•å›¾å¤šæ ‡æ³¨å¯¹æ¯”å±•ç¤º', fontsize=16, fontweight='bold')
            plt.tight_layout()
            
            plt.savefig(f'eval_single_image_{split_name}.png', dpi=200, bbox_inches='tight',
                       facecolor='white', edgecolor='none')
            print(f"ğŸ’¾ å•å›¾å±•ç¤ºå·²ä¿å­˜ä¸º 'eval_single_image_{split_name}.png'")
            
            plt.show()
            break
    
    print("âœ… å•å›¾å¤šæ ‡æ³¨å±•ç¤ºå®Œæˆï¼")

def show_visual_results_diverse_images(model, data_loader, vocab, device, img_dir, split_name, num_samples=15):
    """å±•ç¤ºå¤šå¼ ä¸åŒå›¾ç‰‡ï¼Œæ¯å¼ ä¸€ä¸ªæ ‡æ³¨"""
    print(f"\nğŸ“¸ å¤šå›¾å•æ ‡æ³¨å±•ç¤ºæ¨¡å¼ï¼ˆ{num_samples}å¼ ä¸åŒå›¾ç‰‡ï¼‰...")
    
    model.eval()
    
    # æ”¶é›†ä¸åŒå›¾ç‰‡çš„æ ·æœ¬ - ä½¿ç”¨éšæœºé‡‡æ ·ç¡®ä¿å›¾ç‰‡å¤šæ ·æ€§
    import random
    random.seed(42)  # å›ºå®šéšæœºç§å­ä¿è¯å¯é‡ç°
    
    dataset = data_loader.dataset
    unique_files = list(set([data['filename'] for data in dataset.data]))
    selected_files = random.sample(unique_files, min(num_samples, len(unique_files)))
    
    samples_collected = []
    
    with torch.no_grad():
        # ä¸ºæ¯ä¸ªé€‰ä¸­çš„æ–‡ä»¶ç”Ÿæˆæè¿°
        for filename in selected_files:
            # æ‰¾åˆ°å¯¹åº”çš„æ•°æ®ç´¢å¼•
            data_idx = next(i for i, data in enumerate(dataset.data) if data['filename'] == filename)
            
            # è·å–å›¾åƒå’Œæ ‡æ³¨
            image, caption = dataset[data_idx]
            image = image.unsqueeze(0).to(device)  # æ·»åŠ batchç»´åº¦
            
            # ç”Ÿæˆæè¿°
            sampled_ids = model.sample(image)
            generated = []
            for word_id in sampled_ids[0].cpu().numpy():
                word = vocab.idx2word[word_id]
                if word == '<end>':
                    break
                if word not in ['<start>', '<pad>']:
                    generated.append(word)
            
            # è·å–çœŸå®æè¿°ï¼ˆä»dataset.dataä¸­ç›´æ¥è·å–captionï¼‰
            reference = ' '.join(dataset.data[data_idx]['caption'])
            
            samples_collected.append({
                'filename': filename,
                'generated': ' '.join(generated),
                'reference': reference
            })
    
    print(f"\nğŸ“‹ å¤šå›¾å±•ç¤ºè¯¦æƒ…:")
    
    for i, sample in enumerate(samples_collected):
        print(f"\n  ğŸ“¸ å›¾ç‰‡ {i+1}:")
        print(f"     ğŸ“ æ–‡ä»¶: {sample['filename']}")
        print(f"     ğŸ¤– AIç”Ÿæˆ: {sample['generated']}")
        print(f"     ğŸ‘¨ äººå·¥æ ‡æ³¨: {sample['reference']}")
        
        # ä¸ºæ¯å¼ å›¾åƒåˆ›å»ºå•ç‹¬çš„å›¾è¡¨
        fig, ax = plt.subplots(1, 1, figsize=(12, 10))
        
        # åŠ è½½åŸå§‹å›¾åƒ
        img_path = os.path.join(img_dir, sample['filename'])
        
        try:
            if os.path.exists(img_path):
                original_image = Image.open(img_path).convert('RGB')
                
                # æ˜¾ç¤ºå›¾åƒ
                ax.imshow(original_image)
                ax.axis('off')
                
                # æ·»åŠ ç”Ÿæˆæè¿°å’ŒçœŸå®æè¿° - æ”¯æŒæ¢è¡Œæ˜¾ç¤º
                generated_text = sample['generated']
                reference_text = sample['reference']
                
                # åˆ›å»ºå¯¹æ¯”æ–‡æœ¬ - è®©matplotlibè‡ªåŠ¨æ¢è¡Œ
                comparison_text = f"æ¨¡å‹ç”Ÿæˆæè¿°:\n{generated_text}\n\näººå·¥æ ‡æ³¨:\n{reference_text}"
                
                # æ·»åŠ æ–‡æœ¬æ¡† - åœ¨å›¾åƒä¸‹æ–¹æ˜¾ç¤ºï¼Œå¢å¤§å­—ä½“å’Œæ¡†å¤§å°
                ax.text(0.5, -0.18, comparison_text,
                       transform=ax.transAxes,
                       ha='center', va='top',
                       fontsize=14,
                       bbox=dict(boxstyle="round,pad=1.2", 
                               facecolor="lightblue", 
                               alpha=0.9,
                               edgecolor="darkblue",
                               linewidth=1.5),
                       wrap=True,
                       horizontalalignment='center',
                       verticalalignment='top')
                
                print(f"     âœ… å›¾åƒåŠ è½½æˆåŠŸ")
                
            else:
                ax.text(0.5, 0.5, f"å›¾åƒä¸å­˜åœ¨\n{sample['filename']}",
                       transform=ax.transAxes, ha='center', va='center',
                       fontsize=14, color='red')
                ax.axis('off')

                print(f"     âŒ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨")
                
        except Exception as e:
            ax.text(0.5, 0.5, f"åŠ è½½å¤±è´¥\n{str(e)}",
                   transform=ax.transAxes, ha='center', va='center',
                   fontsize=12, color='red')
            ax.axis('off')
            
            print(f"     âš ï¸ é”™è¯¯: {e}")
        
                 # è°ƒæ•´å¸ƒå±€ - ä¸ºæ›´å¤§çš„æ–‡æœ¬æ¡†ç•™å‡ºæ›´å¤šç©ºé—´
        plt.tight_layout()
        plt.subplots_adjust(bottom=0.3)
        
        # ä¿å­˜å•ç‹¬çš„å›¾åƒ
        plt.savefig(f'eval_image_{i+1}_{split_name}.png', dpi=200, bbox_inches='tight',
                   facecolor='white', edgecolor='none')
        print(f"     ğŸ’¾ å›¾åƒ {i+1} å·²ä¿å­˜ä¸º 'eval_image_{i+1}_{split_name}.png'")
        
        # æ˜¾ç¤ºå›¾åƒ
        plt.show()
        
        # å…³é—­å½“å‰å›¾è¡¨ä»¥é‡Šæ”¾å†…å­˜
        plt.close(fig)
    
    print(f"\nâœ… å¤šå›¾å±•ç¤ºå®Œæˆï¼")

def show_visual_results(model, data_loader, vocab, device, img_dir, split_name, mode='diverse', num_samples=15):
    """å¯è§†åŒ–å±•ç¤ºæ¨¡å‹ç”Ÿæˆæ•ˆæœ
    
    Args:
        mode: 'single' - å•å›¾å¤šæ ‡æ³¨, 'diverse' - å¤šå›¾å•æ ‡æ³¨
        num_samples: å¤šå›¾æ¨¡å¼ä¸‹å±•ç¤ºçš„å›¾ç‰‡æ•°é‡
    """
    
    if mode == 'single':
        show_visual_results_single_image(model, data_loader, vocab, device, img_dir, split_name)
    else:
        show_visual_results_diverse_images(model, data_loader, vocab, device, img_dir, split_name, num_samples)

def evaluate_model(args):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½è¯æ±‡è¡¨
    with open(args.vocab_path, 'rb') as f:
        vocab = pickle.load(f)
    print(f"è¯æ±‡è¡¨å¤§å°: {len(vocab)}")
    
    # åˆ›å»ºæ¨¡å‹å¹¶åŠ è½½æƒé‡
    model = ImageCaptioningModel(
        args.embed_size, args.hidden_size, len(vocab), args.num_layers
    ).to(device)
    
    if os.path.exists(args.model_path):
        model.load_state_dict(torch.load(args.model_path, map_location=device))
        print("åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹")
    else:
        print("è­¦å‘Š: æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
    test_loader = get_data_loader(
        args.json_path, args.img_dir, vocab, args.split, 
        batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers
    )
    
    print(f"åœ¨{args.split}é›†ä¸Šè¯„ä¼°æ¨¡å‹...")
    
    # ç”Ÿæˆæè¿°
    generated_captions, reference_captions = generate_captions_for_eval(
        model, test_loader, vocab, device
    )
    
    print(f"ç”Ÿæˆäº† {len(generated_captions)} ä¸ªæè¿°")
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    print("è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    
    # BLEUåˆ†æ•°
    bleu_scores = calculate_bleu_scores(generated_captions, reference_captions)
    
    # METEORåˆ†æ•°
    meteor = calculate_meteor_scores(generated_captions, reference_captions)
    
    # ROUGEåˆ†æ•°
    rouge_scores = calculate_rouge_scores(generated_captions, reference_captions)
    
    # æ‰“å°ç»“æœ
    print("\n=== è¯„ä¼°ç»“æœ ===")
    for metric, score in bleu_scores.items():
        print(f"{metric}: {score:.4f}")
    
    print(f"METEOR: {meteor:.4f}")
    
    for metric, score in rouge_scores.items():
        print(f"{metric}: {score:.4f}")
    
    # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
    print("\n=== ç”Ÿæˆç¤ºä¾‹ ===")
    for i in range(min(5, len(generated_captions))):
        print(f"ç”Ÿæˆ: {' '.join(generated_captions[i])}")
        print(f"çœŸå®: {' '.join(reference_captions[i][0])}")
        print("-" * 50)
    
    # å¯è§†åŒ–å±•ç¤ºç”Ÿæˆæ•ˆæœ
    show_visual_results(model, test_loader, vocab, device, args.img_dir, args.split, args.visual_mode, num_samples=15)
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    results = {
        **bleu_scores,
        'METEOR': meteor,
        **rouge_scores
    }
    
    with open(f'eval_results_{args.split}.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"è¯„ä¼°ç»“æœä¿å­˜åˆ° eval_results_{args.split}.json")

def main():
    parser = argparse.ArgumentParser(description='æ¨¡å‹è¯„ä¼°')
    
    # æ•°æ®ç›¸å…³å‚æ•°
    parser.add_argument('--json_path', type=str, default='flickr8k_aim3/dataset_flickr8k.json',
                       help='æ•°æ®é›†JSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--img_dir', type=str, default='flickr8k_aim3/images',
                       help='å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('--vocab_path', type=str, default='vocab.pkl',
                       help='è¯æ±‡è¡¨è·¯å¾„')
    parser.add_argument('--model_path', type=str, default='best_model.pth',
                       help='æ¨¡å‹è·¯å¾„')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--embed_size', type=int, default=256,
                       help='åµŒå…¥ç»´åº¦')
    parser.add_argument('--hidden_size', type=int, default=512,
                       help='LSTMéšè—å±‚ç»´åº¦')
    parser.add_argument('--num_layers', type=int, default=1,
                       help='LSTMå±‚æ•°')
    
    # è¯„ä¼°å‚æ•°
    parser.add_argument('--split', type=str, default='test', choices=['train', 'val', 'test'],
                       help='è¯„ä¼°çš„æ•°æ®é›†åˆ†å‰²')
    parser.add_argument('--batch_size', type=int, default=16,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num_workers', type=int, default=0,
                       help='æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°')
    parser.add_argument('--visual_mode', type=str, default='diverse', choices=['single', 'diverse'],
                       help='å¯è§†åŒ–æ¨¡å¼: single-å•å›¾å¤šæ ‡æ³¨, diverse-å¤šå›¾å•æ ‡æ³¨')
    
    args = parser.parse_args()
    evaluate_model(args)

if __name__ == '__main__':
    main() 