import json
import os
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from collections import Counter # è®¡æ•°å™¨ï¼Œç”¨äºç»Ÿè®¡è¯é¢‘
import pickle  # åºåˆ—åŒ–å·¥å…·ï¼Œä¿å­˜/åŠ è½½è¯æ±‡è¡¨
import matplotlib.pyplot as plt
import numpy as np

# è§£å†³OpenMPå†²çªé—®é¢˜
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
matplotlib.rcParams['axes.unicode_minus'] = False

class Vocabulary:
    """è¯æ±‡è¡¨ç±»ï¼Œç”¨äºå¤„ç†æ–‡æœ¬çš„ç¼–ç å’Œè§£ç """
    def __init__(self):
        self.word2idx = {}   #è¯æ±‡â†’ç´¢å¼•çš„æ˜ å°„å­—å…¸
        self.idx2word = {}  #ç´¢å¼•â†’è¯æ±‡çš„æ˜ å°„å­—å…¸
        self.idx = 0        #å½“å‰ç´¢å¼•è®¡æ•°å™¨
        
        # æ·»åŠ ç‰¹æ®Šæ ‡è®°
        self.add_word('<pad>')  # å¡«å……æ ‡è®°ï¼Œç”¨äºç»Ÿä¸€åºåˆ—é•¿åº¦
        self.add_word('<start>')  # å¥å­å¼€å§‹æ ‡è®°ï¼Œå‘Šè¯‰æ¨¡å‹å¥å­å¼€å§‹
        self.add_word('<end>')  # å¥å­ç»“æŸæ ‡è®°ï¼Œ å‘Šè¯‰æ¨¡å‹å¥å­ç»“æŸ
        self.add_word('<unk>')  # æœªçŸ¥è¯æ ‡è®°
        
    def add_word(self, word):
        """æ·»åŠ å•è¯åˆ°è¯æ±‡è¡¨"""
        if word not in self.word2idx:
            self.word2idx[word] = self.idx
            self.idx2word[self.idx] = word
            self.idx += 1
            
    def __call__(self, word):
        """è·å–å•è¯çš„ç´¢å¼•"""
        if word not in self.word2idx:
            return self.word2idx['<unk>']
        return self.word2idx[word]
    
    def __len__(self):
        return len(self.word2idx)

def build_vocab(json_path, threshold=2):   
    """æ„å»ºè¯æ±‡è¡¨"""
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # ç»Ÿè®¡æ‰€æœ‰è¯æ±‡çš„å‡ºç°é¢‘ç‡
    counter = Counter()
    for image in data['images']:
        for sentence in image['sentences']:
            tokens = sentence['tokens']
            counter.update(tokens)
    
    # åªä¿ç•™å‡ºç°æ¬¡æ•°å¤§äºç­‰äºthresholdçš„è¯
    words = [word for word, cnt in counter.items() if cnt >= threshold]
    
    vocab = Vocabulary()
    for word in words:
        vocab.add_word(word)
    
    return vocab

class Flickr8kDataset(Dataset):
    """Flickr8Kæ•°æ®é›†ç±»"""
    def __init__(self, json_path, img_dir, vocab, split='train', transform=None):#ä¸å¯¹å›¾åƒå˜åˆ°åŒä¸€å¤§å°å—ï¼Ÿ
        """
        Args:
            json_path: æ•°æ®é›†JSONæ–‡ä»¶è·¯å¾„
            img_dir: å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
            vocab: è¯æ±‡è¡¨å¯¹è±¡
            split: æ•°æ®é›†åˆ†å‰² ('train', 'val', 'test')
            transform: å›¾åƒé¢„å¤„ç†å˜æ¢
        """
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.img_dir = img_dir
        self.vocab = vocab
        self.transform = transform
        
        # ç­›é€‰æŒ‡å®šsplitçš„æ•°æ®ï¼ŒåŒæ—¶éªŒè¯å›¾åƒæ–‡ä»¶å­˜åœ¨æ€§ï¼ˆæˆ‘å‘ç°æºæ•°æ®é›†é‡Œé¢æœ‰8091å¼ ï¼Œè€Œjsonè®°å½•çš„ä»…æœ‰8000å¼ ï¼‰
        self.data = []
        missing_files = []
        
        for image in data['images']:
            if image['split'] == split:
                img_path = os.path.join(img_dir, image['filename'])
                
                # æ£€æŸ¥å›¾åƒæ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if os.path.exists(img_path):
                    for sentence in image['sentences']:
                        self.data.append({
                            'filename': image['filename'],
                            'caption': sentence['tokens']
                        })
                else:
                    missing_files.append(image['filename'])
        
        # æŠ¥å‘Šç¼ºå¤±æ–‡ä»¶æƒ…å†µ
        if missing_files:
            print(f"æ³¨æ„ï¼š{split}é›†ä¸­æœ‰ {len(missing_files)} ä¸ªå›¾åƒæ–‡ä»¶ç¼ºå¤±ï¼Œå·²è‡ªåŠ¨è·³è¿‡")
            print(f"å®é™…å¯ç”¨çš„ {split} æ•°æ®ï¼š{len(self.data)} ä¸ªå›¾æ–‡å¯¹")
        else:
            print(f"{split}é›†æ•°æ®åŠ è½½å®Œæˆï¼š{len(self.data)} ä¸ªå›¾æ–‡å¯¹")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # åŠ è½½å›¾åƒ
        img_path = os.path.join(self.img_dir, item['filename'])
        image = Image.open(img_path).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
        
        # å¤„ç†æ ‡é¢˜æ–‡æœ¬
        caption = item['caption']
        caption = ['<start>'] + caption + ['<end>']
        
        # è½¬æ¢ä¸ºç´¢å¼•
        caption_indices = [self.vocab(word) for word in caption]
        caption_tensor = torch.tensor(caption_indices, dtype=torch.long)
        
        return image, caption_tensor

def collate_fn(batch):#PyTorchè¦æ±‚æ‰¹æ¬¡ä¸­å¼ é‡å½¢çŠ¶ä¸€è‡´ - éœ€è¦å°†ä¸åŒé•¿åº¦åºåˆ—å¡«å……åˆ°ç›¸åŒé•¿åº¦
    """è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°ï¼Œå¤„ç†ä¸åŒé•¿åº¦çš„æ ‡é¢˜"""
    # æŒ‰æ ‡é¢˜é•¿åº¦æ’åº
    batch.sort(key=lambda x: len(x[1]), reverse=True)#LSTMçš„pack_padded_sequenceè¦æ±‚æŒ‰é•¿åº¦é™åº - æé«˜RNNå¤„ç†æ•ˆç‡ï¼Œå‡å°‘æ— æ•ˆè®¡ç®—
    
    images, captions = zip(*batch)
    
    # å †å å›¾åƒ
    images = torch.stack(images, 0)# ä¸ºå•¥å›¾åƒå½¢çŠ¶å·²ç»Ÿä¸€ï¼Œç›´æ¥å †å ï¼Ÿï¼Ÿï¼Ÿ
    
    # å¡«å……æ ‡é¢˜åˆ°ç›¸åŒé•¿åº¦
    lengths = [len(cap) for cap in captions]
    targets = torch.zeros(len(captions), max(lengths)).long() # åˆ›å»ºå¡«å……çŸ©é˜µ
    
    for i, cap in enumerate(captions):
        end = lengths[i]
        targets[i, :end] = cap[:end]
    
    return images, targets, lengths

def get_data_loader(json_path, img_dir, vocab, split='train', batch_size=32, shuffle=True, num_workers=0):
    """è·å–æ•°æ®åŠ è½½å™¨"""
    # å›¾åƒé¢„å¤„ç†
    if split == 'train':
        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.RandomHorizontalFlip(0.5),# 50%æ¦‚ç‡æ°´å¹³ç¿»è½¬ (æ•°æ®å¢å¼º)
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
    else:
        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
    
    dataset = Flickr8kDataset(json_path, img_dir, vocab, split, transform)
    
    data_loader = DataLoader(
        dataset=dataset,
        batch_size=batch_size,
        shuffle=shuffle,# æ˜¯å¦æ‰“ä¹± (è®­ç»ƒæ—¶Trueï¼Œæµ‹è¯•æ—¶False)
        num_workers=num_workers, # å¤šè¿›ç¨‹åŠ è½½ (Windowså»ºè®®è®¾ä¸º0)
        collate_fn=collate_fn
    )
    
    return data_loader

if __name__ == '__main__':
    # æµ‹è¯•ä»£ç 
    json_path = 'flickr8k_aim3/dataset_flickr8k.json'
    img_dir = 'flickr8k_aim3/images'
    
    # æ„å»ºè¯æ±‡è¡¨
    vocab = build_vocab(json_path)
    print(f"è¯æ±‡è¡¨å¤§å°: {len(vocab)}")
    
    # ä¿å­˜è¯æ±‡è¡¨
    with open('vocab.pkl', 'wb') as f:
        pickle.dump(vocab, f)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = get_data_loader(json_path, img_dir, vocab, 'train', batch_size=4)
    
    # è·å–æ•°æ®é›†å¯¹è±¡ä»¥ä¾¿åç»­è·å–åŸå§‹æ•°æ®
    train_dataset = train_loader.dataset
    
    # æµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
    print("\n=== æ‰¹æ¬¡æ•°æ®å±•ç¤º ===")
    for batch_idx, (images, captions, lengths) in enumerate(train_loader):
        print(f"ğŸ“Š æ‰¹æ¬¡ {batch_idx + 1} åŸºæœ¬ä¿¡æ¯:")
        print(f"   å›¾åƒå¼ é‡å½¢çŠ¶: {images.shape}")
        print(f"   æ ‡é¢˜å¼ é‡å½¢çŠ¶: {captions.shape}")
        print(f"   åºåˆ—é•¿åº¦åˆ—è¡¨: {lengths}")
        
        print(f"\nğŸ“‹ æ‰¹æ¬¡å†…å®¹è¯¦æƒ…:")
        batch_size = images.shape[0]
        
        # ä»è®­ç»ƒé›†ä¸­éšæœºé€‰æ‹©4å¼ ä¸åŒçš„å›¾åƒ
        import random
        random.seed(42)  # å›ºå®šéšæœºç§å­
        
        # è·å–æ‰€æœ‰å”¯ä¸€çš„å›¾åƒæ–‡ä»¶å
        unique_images = {}
        for idx, sample in enumerate(train_dataset.data):
            filename = sample['filename']
            if filename not in unique_images:
                unique_images[filename] = idx  # ä¿å­˜ç¬¬ä¸€æ¬¡å‡ºç°çš„ç´¢å¼•
        
        # éšæœºé€‰æ‹©4å¼ ä¸åŒå›¾åƒ
        selected_files = random.sample(list(unique_images.keys()), min(4, len(unique_images)))
        
        print(f"ğŸ² éšæœºé€‰æ‹©çš„å›¾åƒæ–‡ä»¶:")
        for i, filename in enumerate(selected_files):
            print(f"   {i+1}. {filename}")
        
        # åˆ›å»ºå›¾åƒæ˜¾ç¤ºçª—å£
        fig, axes = plt.subplots(2, 2, figsize=(16, 10))
        axes = axes.flatten()
        
        print(f"\nğŸ“‹ æ ·æœ¬è¯¦æƒ…:")
        
        for i, filename in enumerate(selected_files):
            print(f"\n  ğŸ”¸ å›¾åƒ {i+1}:")
            print(f"     ğŸ“ æ–‡ä»¶: {filename}")
            
            # è·å–è¯¥å›¾åƒçš„ç¬¬ä¸€ä¸ªæè¿°
            sample_idx = unique_images[filename]
            image, caption_tensor = train_dataset[sample_idx]
            
            # è§£ç æè¿°
            caption_indices = caption_tensor.tolist()
            caption_words = []
            for idx in caption_indices:
                word = vocab.idx2word[idx]
                if word not in ['<pad>']:
                    caption_words.append(word)
            
            # å»é™¤ç‰¹æ®Šæ ‡è®°çš„å¹²å‡€æè¿°
            clean_caption = ' '.join([w for w in caption_words if w not in ['<start>', '<end>']])
            print(f"     ğŸ“ æè¿°: {clean_caption}")
            
            # åŠ è½½åŸå§‹å›¾åƒ
            img_path = os.path.join(img_dir, filename)
            
            try:
                if os.path.exists(img_path):
                    original_image = Image.open(img_path).convert('RGB')
                    
                    # æ˜¾ç¤ºå›¾åƒï¼ˆä¸æ˜¾ç¤ºæ ‡é¢˜ï¼Œé¿å…é‡å¤ï¼‰
                    axes[i].imshow(original_image)
                    axes[i].axis('off')  # ç§»é™¤åæ ‡è½´
                    
                    # åœ¨å›¾åƒä¸‹æ–¹æ·»åŠ æè¿°æ–‡æœ¬
                    if len(clean_caption) > 85:
                        display_caption = clean_caption[:85] + "..."
                    else:
                        display_caption = clean_caption
                    
                    axes[i].text(0.5, -0.08, display_caption,
                               transform=axes[i].transAxes,
                               ha='center', va='top',
                               fontsize=11, wrap=True,
                               bbox=dict(boxstyle="round,pad=0.6", 
                                       facecolor="lightsteelblue", 
                                       alpha=0.9,
                                       edgecolor="steelblue",
                                       linewidth=1))
                    
                    print(f"     âœ… åŠ è½½æˆåŠŸ")
                else:
                    axes[i].text(0.5, 0.5, f"å›¾åƒä¸å­˜åœ¨",
                               transform=axes[i].transAxes, ha='center', va='center',
                               fontsize=14, color='red')
                    axes[i].axis('off')
                    print(f"     âŒ æ–‡ä»¶ä¸å­˜åœ¨")
                    
            except Exception as e:
                print(f"     âš ï¸  é”™è¯¯: {e}")
                axes[i].text(0.5, 0.5, f"åŠ è½½å¤±è´¥",
                           transform=axes[i].transAxes, ha='center', va='center',
                           fontsize=14, color='red')
                axes[i].axis('off')
            
            print(f"     {'-'*60}")
        
        # ä¼˜åŒ–å¸ƒå±€
        plt.tight_layout()
        plt.subplots_adjust(top=0.92, bottom=0.15, hspace=0.3, wspace=0.1)
        
        # ä¿å­˜å›¾åƒ
        plt.savefig('dataset_samples.png', dpi=200, bbox_inches='tight', 
                   facecolor='white', edgecolor='none')
        print(f"\nğŸ’¾ æ•°æ®é›†æ ·æœ¬å±•ç¤ºå·²ä¿å­˜ä¸º 'dataset_samples.png'")
        
        # æ˜¾ç¤ºå›¾åƒ
        plt.show()
        
        print(f"\nâœ… å¤šæ ·åŒ–å›¾åƒå±•ç¤ºå®Œæˆï¼")
        
        break 